{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uUBS9Hww6eb0",
        "4cL4DuFif3i1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "5rVwyxa1SbHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k beautifulsoup4 readability-lxml tiktoken mistralai openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iiqs_-x6SbB2",
        "outputId": "cee5a764-352f-4d46-da1b-9d18399490dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting readability-lxml\n",
            "  Downloading readability_lxml-0.8.1-py3-none-any.whl (20 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mistralai\n",
            "  Downloading mistralai-0.1.3-py3-none-any.whl (15 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from readability-lxml) (5.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Collecting httpx<0.26.0,>=0.25.2 (from mistralai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.10 (from mistralai)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<3.0.0,>=2.2.0 (from mistralai)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow<16.0.0,>=15.0.0 (from mistralai)\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from mistralai) (2.6.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.26.0,>=0.25.2->mistralai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<0.26.0,>=0.25.2->mistralai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.26.0,>=0.25.2->mistralai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.2.0->mistralai) (1.25.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.2.0->mistralai) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas<3.0.0,>=2.2.0->mistralai)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->mistralai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=119e5bb296e6c85a9e6332794e9289ee588cecc2a49d53ed00fd2430dd574577\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=f202bfb9bb3b7a18c7f447d4fa869a396397a10cbd4f83e0947ba0a130bd8408\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398381 sha256=0265aeada63931194d4721c6bed768dfede32d3bc1c553856ac2cdc8aec32edd\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=7b8512ffe5ef8744266d0d31f0b2c4ea77a7561fd281144c450fde1dd66113fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, tzdata, pyarrow, orjson, h11, feedparser, cssselect, tiktoken, requests-file, readability-lxml, pandas, httpcore, feedfinder2, tldextract, httpx, openai, newspaper3k, mistralai\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.21.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 h11-0.14.0 httpcore-1.0.4 httpx-0.25.2 jieba3k-0.35.1 mistralai-0.1.3 newspaper3k-0.2.8 openai-1.13.3 orjson-3.9.15 pandas-2.2.1 pyarrow-15.0.0 readability-lxml-0.8.1 requests-file-2.0.0 sgmllib3k-1.0.0 tiktoken-0.6.0 tinysegmenter-0.3 tldextract-5.1.1 tzdata-2024.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SPARQLWrapper networkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1urRcPrhsL7H",
        "outputId": "251c86f0-6560-4e39-dfc7-5d1f306dcdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib, SPARQLWrapper\n",
            "Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-7.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nk0ROCwLjrA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import concurrent.futures\n",
        "import http.client\n",
        "import json\n",
        "from json import JSONDecodeError\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O117b0FdhHYJ",
        "outputId": "fbd0d54e-b183-4494-c5e6-28214004c45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "  encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "  return len(encoding.encode(text))"
      ],
      "metadata": {
        "id": "0IMkXVFtpLZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "mistral_api_key = userdata.get('MISTRAL_API_KEY')\n",
        "mistral_client = MistralClient(api_key=mistral_api_key)"
      ],
      "metadata": {
        "id": "jtngb2uH7z7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "oai_api_key = userdata.get('OPENAI_KEY')\n",
        "oai_client = OpenAI(api_key=oai_api_key)"
      ],
      "metadata": {
        "id": "O2aPtBqlHWA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword Generation\n",
        "\n",
        "Generates a list of keywords from natural language query submitted by the user"
      ],
      "metadata": {
        "id": "4e5JFYHAfzT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "MAX_ANSWER_TOKENS = 13000\n",
        "\n",
        "# KEYWORD_SYSTEM_PROMPT = '''\n",
        "# <s>\n",
        "# [INST]\n",
        "# You are an artificial intelligence model created by Devflow Studio experienced in natural language processing. You are tasked with helping your user perform better searches through a search engine. The user will provide you a raw query that you need to polish and rewrite to get best search results from a search engine. Given a user query, you are tasked with generating a set of keywords suitable for a search engine query, with the added capability to discern and specify the type of search required — be it a web search, a place search, or a news search — based on the query's context and intent. This feature, along with enhancing capabilities for image search, accommodating local searches, and generating an entity for the Knowledge Graph based on WikiData, forms a comprehensive approach to query processing. The response should be structured as a JSON object containing the following keys based on the query's context and requirements:\n",
        "\n",
        "# 1. **`queries`**: An array of 1 to 3 keywords encapsulating the main topics or questions within the query, prioritizing brevity and relevance. Breakdown the question into parts to get best results. For simple and straightforward questions, you can simply create a single query in an array. For more complex queries that might require extensive research, break down each question from the query into simpler parts. Make sure to use your training knowledge to write keywords that yield best results from a search engine.\n",
        "\n",
        "# 2. **`type`**: Determine whether the query should lead to a `websearch`, `place`, or `news` search based on the user's intent and the nature of the query. This type classification guides the search engine on how to best fulfill the query.\n",
        "\n",
        "# 3. **`image_search`**: (Optional, defaults to false) Use image search if the query's context suggests that visual results would be beneficial. You use image search when you believe that visual aid may help answer user's query better or might result in more engagement. Image search shall be used for queries about people, places / landmarks, objects (things), products, etc.\n",
        "\n",
        "# 4. **`entity`**: (Optional) When the main subject of the query is about a person, place, or organization/company, include this field with the name of the entity as recognized by Wikipedia for Knowledge Graph generation. Knowledge Graphs are rendered in the frontend with a visually appealing UI. The Knowledge Graph will be help the user learn in an engaging way rather than reading just text.\n",
        "\n",
        "# **Guidelines for the Model:**\n",
        "\n",
        "# - **Determining the Search Type:**\n",
        "#    - **Web Search (`websearch`)**: Default for queries seeking general information, explanations, guides, or knowledge not tied to a specific location or recent events.\n",
        "#    - **Place Search (`place`)**: For queries specifically asking for local services, businesses, restaurants, hotels or attractions, indicating the need for localized information.\n",
        "#    - **News Search (`news`)**: When the query is about current events, updates, or recent developments, necessitating the latest news coverage.\n",
        "\n",
        "# - **When to Search for Images:** Initiate an image search for queries where visuals significantly enhance understanding or engagement, focusing on people, places/landmarks, products, etc.\n",
        "\n",
        "# - **Utilizing User Location:** Use the user's location for \"near me\" requests or when local context is vital, adapting keywords to include local specificity.\n",
        "\n",
        "# - **Identifying Entities for Knowledge Graph:** Focus on accurately identifying entities for people, places, or organizations. For ambiguous references or multiple entities, prioritize based on query context and use disambiguation strategies. The entity must be recognised by Wikipedia to be able to generate a Knowledge Graph for it. Make sure to write a complete and proper entity name. For example: when user wants to know about manufacturer of iPhone, \"Apple\" will result in the fruit whereas \"Apple Inc\" will result in the Cupertino-based company.\n",
        "\n",
        "# Strictly adhere to responding in JSON format, ensuring clarity, brevity, and adherence to the outlined format.\n",
        "# Try your best at the given task. Even if there's info missing, try your best to build a query out of it. You are not allowed to ask question. You are only tasked with responding to the user and not backforth communication.\n",
        "\n",
        "# Here's an example:\n",
        "# User Query: \"Where are some places I can visit nearby\"\n",
        "# User Location: \"Seattle, Washington\"\n",
        "# [/INST]\n",
        "# {\"queries\": [\"tourist spots seattle washington\"], \"type\": \"place\", \"image_search\": true, \"entity\": \"Seattle\"}\n",
        "# [INST]\n",
        "# Here's another example:\n",
        "# User Query: \"Who is the CEO of Tesla\"\n",
        "# [/INST]\n",
        "# {\"queries\": [\"Tesla CEO\"], \"type\": \"websearch\", \"image_search\": true}\n",
        "# [INST]\n",
        "# Here's another example:\n",
        "# User Query: \"Who is Sam Altman\"\n",
        "# [/INST]\n",
        "# {\"queries\": [\"Sam Altman\", \"Sam Altman biography\"], \"type\": \"websearch\", \"image_search\": true, \"entity\": \"Sam Altman\"}\n",
        "# </s>\n",
        "# '''\n",
        "\n",
        "# SEARCH_BUILDER_MESSAGES = [\n",
        "#   ('system', KEYWORD_SYSTEM_PROMPT),\n",
        "#   ('user', 'User Query: \"How does photosynthesis work?\"'),\n",
        "#   ('assistant', '{\"queries\": [\"photosynthesis process\"], \"type\": \"websearch\"}'),\n",
        "#   ('user', 'User Query: \"Best Italian restaurants near me\"\\nUser Location: San Francisco'),\n",
        "#   ('assistant', '{\"queries\": [\"Italian restaurants San Francisco\"], \"type\": \"place\"}'),\n",
        "#   ('user', 'User Query: \"Latest updates on elections\"\\nUser Location: New Delhi'),\n",
        "#   ('assistant', '{\"queries\": [\"election news new delhi\", \"election new india\"], \"type\": \"news\"}'),\n",
        "#   ('user', 'User Query: \"Show me pictures of the Taj Mahal at sunrise\"'),\n",
        "#   ('assistant', '{\"queries\": [\"Taj Mahal sunrise pictures\"], \"image\": {\"queries\": [\"Taj Mahal sunrise\"]}, \"entity\": \"Taj Mahal\", \"type\": \"websearch\"}'),\n",
        "#   ('user', 'User Query: \"What are the best places to visit near me?\"\\nUser Location: New York'),\n",
        "#   ('assistant', '{\"queries\": [\"tourist spots new york\"], \"type\": \"place\"}')\n",
        "# ]\n",
        "\n",
        "# def build_search(query: str, location: str | None = None, attempt: int = 1) -> dict:\n",
        "#   try:\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     query = f\"User: '{query}'\"\n",
        "#     if location:\n",
        "#       query += f\"\\nUser Location: {location}\"\n",
        "\n",
        "#     messages = []\n",
        "\n",
        "#     for role, content in SEARCH_BUILDER_MESSAGES:\n",
        "#       messages.append(ChatMessage(role=role, content=content))\n",
        "\n",
        "#     user_message = f'User Query: \"{query}\"'\n",
        "#     if location:\n",
        "#       user_message += f'\\nUserLocation: {location}'\n",
        "\n",
        "#     messages.append(ChatMessage(role=\"user\", content=user_message))\n",
        "\n",
        "#     response = mistral_client.chat(\n",
        "#       model=\"open-mistral-7b\",\n",
        "#       messages=messages,\n",
        "#     )\n",
        "\n",
        "#     print(f\"Generated keywords in {time.time() - start_time} seconds\")\n",
        "\n",
        "#     return json.loads(response.choices[0].message.content)\n",
        "#   except Exception as e:\n",
        "#     if attempt > 1:\n",
        "#       print(f'Attempt {attempt} resulted in error {e}')\n",
        "#     else:\n",
        "#       print(f'Error occured building search info {e}')\n",
        "#     if attempt < 5:\n",
        "#       return build_search(query, location, attempt + 1)\n",
        "#     else:\n",
        "#       return None\n",
        "\n",
        "KEYWORD_SYSTEM_PROMPT = '''\n",
        "You are an artificial intelligence model created by Devflow Studio experienced in natural language processing. You are tasked with helping your user perform better searches through a search engine. The user will provide you a raw query that you need to polish and rewrite to get best search results from a search engine. Given a user query, you are tasked with generating a set of keywords suitable for a search engine query, with the added capability to discern and specify the type of search required — be it a web search, a place search, or a news search — based on the query's context and intent. This feature, along with enhancing capabilities for image search, accommodating local searches, and generating an entity for the Knowledge Graph based on WikiData, forms a comprehensive approach to query processing. The response should be structured as a JSON object containing the following keys based on the query's context and requirements:\n",
        "\n",
        "1. **`queries`**: An array of 1 to 3 keywords encapsulating the main topics or questions within the query, prioritizing brevity and relevance. Breakdown the question into parts to get best results. For simple and straightforward questions, you can simply create a single query in an array. For more complex queries that might require extensive research, break down each question from the query into simpler parts. Make sure to use your training knowledge to write keywords that yield best results from a search engine.\n",
        "\n",
        "2. **`type`**: Determine whether the query should lead to a `websearch`, `place`, or `news` search based on the user's intent and the nature of the query. This type classification guides the search engine on how to best fulfill the query.\n",
        "\n",
        "3. **`image_search`**: (Optional, defaults to false) Use image search if the query's context suggests that visual results would be beneficial. You use image search when you believe that visual aid may help answer user's query better or might result in more engagement. Image search shall be used for queries about people, places / landmarks, objects (things), products, etc.\n",
        "\n",
        "4. **`entity`**: (Optional) When the main subject of the query is about a person, place, or organization/company, include this field with the name of the entity as recognized by Wikipedia for Knowledge Graph generation. Knowledge Graphs are rendered in the frontend with a visually appealing UI. The Knowledge Graph will be help the user learn in an engaging way rather than reading just text.\n",
        "\n",
        "**Guidelines for the Model:**\n",
        "\n",
        "- **Determining the Search Type:**\n",
        "   - **Web Search (`websearch`)**: Default for queries seeking general information, explanations, guides, or knowledge not tied to a specific location or recent events.\n",
        "   - **Place Search (`place`)**: For queries specifically asking for local services, businesses, restaurants, hotels or attractions, indicating the need for localized information.\n",
        "   - **News Search (`news`)**: When the query is about current events, updates, or recent developments, necessitating the latest news coverage.\n",
        "\n",
        "- **When to Search for Images:** Initiate an image search for queries where visuals significantly enhance understanding or engagement, focusing on people, places/landmarks, products, etc.\n",
        "\n",
        "- **Utilizing User Location:** Use the user's location for \"near me\" requests or when local context is vital, adapting keywords to include local specificity.\n",
        "\n",
        "- **Identifying Entities for Knowledge Graph:** Focus on accurately identifying entities for people, places, or organizations. For ambiguous references or multiple entities, prioritize based on query context and use disambiguation strategies. The entity must be recognised by Wikipedia to be able to generate a Knowledge Graph for it. Make sure to write a complete and proper entity name. For example: when user wants to know about manufacturer of iPhone, \"Apple\" will result in the fruit whereas \"Apple Inc\" will result in the Cupertino-based company.\n",
        "\n",
        "Strictly adhere to responding in JSON format, ensuring clarity, brevity, and adherence to the outlined format.\n",
        "Try your best at the given task. Even if there's info missing, try your best to build a query out of it. You are not allowed to ask question. You are only tasked with responding to the user and not backforth communication.\n",
        "\n",
        "Here are a few examples:\n",
        "\n",
        "User Query: \"Where are some places I can visit nearby\"\n",
        "User Location: \"Seattle, Washington\"\n",
        "Expected Function Call: {\"keywords\": [\"tourist spots seattle washington\"], \"search_type\": \"places\", \"image_search\": true, \"entity\": \"Seattle\"}\n",
        "\n",
        "User Query: \"Who is the CEO of Tesla\"\n",
        "Expected Function Call: {\"keywords\": [\"Tesla CEO\"], \"search_type\": \"websearch\", \"image_search\": true}\n",
        "\n",
        "User Query: \"Who is Sam Altman\"\n",
        "Expected Function Call: {\"keywords\": [\"Sam Altman\", \"Sam Altman biography\"], \"search_type\": \"websearch\", \"image_search\": true, \"entity\": \"Sam Altman\"}\n",
        "\n",
        "User Query: \"How does photosynthesis work?\"\n",
        "Expected Function Call: {\"keywords\": [\"photosynthesis process\"], \"search_type\": \"websearch\"}\n",
        "\n",
        "User Query: \"Best Italian restaurants near me\"\n",
        "User Location: San Francisco\n",
        "Expected Function Call: {\"keywords\": [\"Italian restaurants San Francisco\"], \"search_type\": \"places\"}\n",
        "\n",
        "User Query: \"Latest updates on elections\"\n",
        "User Location: New Delhi\n",
        "Expected Function Call: {\"keywords\": [\"election news new delhi\", \"election new india\"], \"search_type\": \"news\"}\n",
        "\n",
        "User Query: \"Show me pictures of the Taj Mahal at sunrise\"\n",
        "Expected Function Call: {\"keywords\": [\"Taj Mahal sunrise pictures\"], \"image_search\": true, \"entity\": \"Taj Mahal\", \"search_type\": \"websearch\"}\n",
        "\n",
        "User Query: \"What are the best places to visit near me?\"\n",
        "User Location: New York\n",
        "Expected Function Call: {\"keywords\": [\"tourist spots new york\"], \"image_search\": true, \"search_type\": \"place\"}\n",
        "'''\n",
        "\n",
        "def build_search(query: str, location: str | None = None, attempt: int = 1) -> dict:\n",
        "  try:\n",
        "    tool = {\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\n",
        "        \"name\": \"build_search_info\",\n",
        "        \"description\": \"Generates search info for search engine\",\n",
        "        \"parameters\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"keywords\": {\n",
        "              \"type\": \"array\",\n",
        "              \"description\": \"An array of 1 to 3 keywords encapsulating the main topics or questions within the query, prioritizing brevity and relevance. Breakdown the question into parts to get best results. For simple and straightforward questions, you can simply create a single query in an array. For more complex queries that might require extensive research, break down each question from the query into simpler parts. Make sure to use your training knowledge to write keywords that yield best results from a search engine.\",\n",
        "              \"items\": {\n",
        "                \"type\": \"string\"\n",
        "              }\n",
        "            },\n",
        "            \"search_type\": {\n",
        "              \"type\": \"string\",\n",
        "              \"enum\": [\"websearch\", \"news\", \"places\"],\n",
        "              \"description\": \"Determine whether the query should lead to a `websearch`, `places`, or `news` search based on the user's intent and the nature of the query. This type classification guides the search engine on how to best fulfill the query.\"\n",
        "            },\n",
        "            \"image_search\": {\n",
        "              \"type\": \"boolean\",\n",
        "              \"description\": \"Use image search if the query's context suggests that visual results would be beneficial. You use image search when you believe that visual aid may help answer user's query better or might result in more engagement. Image search shall be used for queries about people, places / landmarks, objects (things), products, etc.\"\n",
        "            },\n",
        "            \"entity\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"When the main subject of the query is about a person, place, or organization/company, include this field with the name of the entity as recognized by Wikipedia for Knowledge Graph generation. Knowledge Graphs are rendered in the frontend with a visually appealing UI. The Knowledge Graph will be help the user learn in an engaging way rather than reading just text.\"\n",
        "            }\n",
        "          },\n",
        "          \"required\": [\"queries\", \"search_type\"],\n",
        "        },\n",
        "      },\n",
        "    }\n",
        "\n",
        "    user_message = f'User Query: \"{query}\"'\n",
        "    if location:\n",
        "      user_message += f'\\nUserLocation: {location}'\n",
        "\n",
        "    response = oai_client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo-0125\",\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": KEYWORD_SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": user_message\n",
        "        },\n",
        "      ],\n",
        "      tools=[tool],\n",
        "      tool_choice={\"type\": \"function\", \"function\": {\"name\": \"build_search_info\"}}\n",
        "    )\n",
        "\n",
        "    func = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
        "\n",
        "    return func\n",
        "  except Exception as e:\n",
        "    if attempt > 1:\n",
        "      print(f'Attempt {attempt} resulted in error {e}')\n",
        "    else:\n",
        "      print(f'Error occured building search info {e}')\n",
        "    if attempt < 5:\n",
        "      return build_search(query, location, attempt + 1)\n",
        "    else:\n",
        "      return None"
      ],
      "metadata": {
        "id": "4CrS3uR3FnR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_search(\"Tourist spots nearby\", location=\"Shimla\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNVukLPVK4Cg",
        "outputId": "61b2e14e-46f0-449d-dbe1-30b268acbb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occured building search info name 'OPENAI_KEYWORD_SYSTEM_PROMPT' is not defined\n",
            "Attempt 2 resulted in error name 'OPENAI_KEYWORD_SYSTEM_PROMPT' is not defined\n",
            "Attempt 3 resulted in error name 'OPENAI_KEYWORD_SYSTEM_PROMPT' is not defined\n",
            "Attempt 4 resulted in error name 'OPENAI_KEYWORD_SYSTEM_PROMPT' is not defined\n",
            "Attempt 5 resulted in error name 'OPENAI_KEYWORD_SYSTEM_PROMPT' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Graph\n",
        "\n",
        "Build knowledge graphs for website and mobile app that display the info in an engaging way similar to Google's Knowledge Panels.\n",
        "\n",
        "Use-cases:\n",
        "\n",
        "1. People\n",
        "2. Places\n",
        "3. Business Profiles (Places, Organisations)"
      ],
      "metadata": {
        "id": "uUBS9Hww6eb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_dict(input_dict: dict) -> dict:\n",
        "  \"\"\"\n",
        "  Removes keys from a dictionary where the corresponding value is None.\n",
        "\n",
        "  :param input_dict: Dictionary to be cleaned\n",
        "  :return: A new dictionary with all keys with None values removed\n",
        "  \"\"\"\n",
        "  # Create a copy of the keys list to safely modify the dict while iterating\n",
        "  for key in list(input_dict.keys()):\n",
        "    # Remove the key if the value is None or an empty string\n",
        "    if type(input_dict[key]) is list and len(input_dict[key]) == 1:\n",
        "      input_dict[key] = input_dict[key][0]\n",
        "    if not input_dict[key] or input_dict[key] == \"\":\n",
        "      del input_dict[key]\n",
        "  return input_dict\n",
        "\n",
        "def fix_links(graph: dict) -> dict:\n",
        "  '''\n",
        "  Replaces the Facebook, Instagram and Twitter and LinkedIn usernames with their links\n",
        "\n",
        "  :param graph: Graph generated by the function\n",
        "  :return: Graph modified by replacing the usernames with links\n",
        "  '''\n",
        "\n",
        "  social_networks = {\n",
        "    'facebook': 'https://www.facebook.com/{}',\n",
        "    'instagram': 'https://instagram.com/{}',\n",
        "    'twitter': 'https://twitter.com/{}',\n",
        "    'linkedin': 'https://linkedin.com/in/{}',\n",
        "  }\n",
        "\n",
        "  graph['social_links'] = {}\n",
        "\n",
        "  for network, url_format in social_networks.items():\n",
        "    if network in graph:\n",
        "      username = graph[network]\n",
        "      if type(username) is list:\n",
        "        username = username[0]\n",
        "      graph['social_links'][network] = url_format.format(username)\n",
        "      del graph[network]\n",
        "\n",
        "  return graph"
      ],
      "metadata": {
        "id": "nK7cwh3t6c_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_entity(entity: str, language: str = \"en\"):\n",
        "  \"\"\"\n",
        "  Search for a Wikidata entity by query and return its Q number.\n",
        "\n",
        "  Parameters:\n",
        "  query (str): The search query to find the entity on Wikidata.\n",
        "\n",
        "  Returns:\n",
        "  str: The Q number of the first matching entity, or None if no match is found.\n",
        "  \"\"\"\n",
        "\n",
        "  wikidata_search_url = \"https://www.wikidata.org/w/api.php\"\n",
        "  params = {\n",
        "    \"action\": \"wbsearchentities\",\n",
        "    \"language\": language,\n",
        "    \"format\": \"json\",\n",
        "    \"search\": entity\n",
        "  }\n",
        "  try:\n",
        "    response = requests.get(wikidata_search_url, params=params)\n",
        "    response.raise_for_status()  # Raises HTTPError for bad responses\n",
        "\n",
        "    # Parse the JSON response\n",
        "    data = response.json()\n",
        "\n",
        "    # Check if search results exist, if yes then return the first result\n",
        "    if data['search']:\n",
        "      return data['search'][0]\n",
        "    else:\n",
        "      return None\n",
        "  except requests.exceptions.HTTPError as http_err:\n",
        "    print(f\"HTTP error occurred: {http_err}\")\n",
        "    return None\n",
        "  except requests.exceptions.RequestException as err:\n",
        "    print(f\"Request exception: {err}\")\n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "# query = \"openai\"\n",
        "# entity = search_entity(query)\n",
        "# q_number = entity['id']\n",
        "# label = entity['display']['label']['value']\n",
        "# description = entity['display']['description']['value']\n",
        "# print(f\"Q number for '{query}': {q_number}\")\n",
        "# print(label)\n",
        "# print(description)"
      ],
      "metadata": {
        "id": "SsJrZodEsOf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import networkx as nx\n",
        "\n",
        "def get_knowledge_graph(q_number, language: str = \"en\"):\n",
        "  try:\n",
        "    # Configure the SPARQL query endpoint\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "\n",
        "    # SPARQL query to fetch properties and values for the given person\n",
        "    query = f\"\"\"\n",
        "    SELECT ?property ?propertyLabel ?value ?valueLabel\n",
        "    WHERE {{\n",
        "      wd:{q_number} ?property ?value .\n",
        "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"{language}\". }}\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    results = sparql.query().convert()\n",
        "\n",
        "    if len(results['results']['bindings']) <= 0:\n",
        "      return None\n",
        "\n",
        "    property_labels = {\n",
        "      'P18': 'image',\n",
        "      'P2002': 'twitter',\n",
        "      'P2013': 'facebook',\n",
        "      'P2003': 'instagram',\n",
        "      'P6634': 'linkedin',\n",
        "      'P856': 'website',\n",
        "    }\n",
        "\n",
        "    attribute_labels = {\n",
        "      'P106': 'Occupation',              # Person\n",
        "      'P101': 'Field of Work',           # Person\n",
        "      'P800': 'Notable Work',            # Person / Organisation\n",
        "      'P27': 'Country of Citizenship',   # Person\n",
        "      'P27': 'Nationality',              # Person\n",
        "      'P569': 'Date of Birth',           # Person\n",
        "      'P22': 'Father',                   # Person\n",
        "      'P25': 'Mother',                   # Person\n",
        "      'P3373': 'Siblings',               # Person\n",
        "      'P26': 'Spouse',                   # Person\n",
        "      'P40': 'Children',                 # Person\n",
        "      'P2218': 'Net Worth',              # Person\n",
        "      'P108': 'Employer',                # Person\n",
        "      'P39': 'Position Held',            # Person\n",
        "      'P2048': 'Height',                 # Person\n",
        "      'P21': 'Gender',                   # Person\n",
        "      'P102': 'Political Affiliation',   # Person\n",
        "      'P69': 'Educated at',              # Person\n",
        "\n",
        "      'P131': 'City',                    # Place\n",
        "      'P1376': 'Capital Of',             # Place\n",
        "      'P2046': 'Area',                   # Place\n",
        "      'P1082': 'Population',             # Place\n",
        "      'P36': 'Capital',                  # Place\n",
        "      'P6': 'Head of Government',        # Place\n",
        "      'P571': 'Inception',               # Place\n",
        "\n",
        "      'P101': 'Field of Work',           # Organisation (Company)\n",
        "      'P169': 'CEO',                     # Organisation (Company)\n",
        "      'P452': 'Industry',                # Organisation (Company)\n",
        "      'P159': 'Headquarters',            # Organisation (Company)\n",
        "      'P112': 'Founded by',              # Organisation (Company) / Place\n",
        "      'P3320': 'Board Members',          # Organisation (Company)\n",
        "      'P17': 'Country',                  # Organisation (Company) / Place\n",
        "      'P1128': 'Employees',              # Organisation (Company)\n",
        "\n",
        "      'P166': 'Award Received',          # Person / Organisation / Place\n",
        "      'P1830': 'Owner of',               # Person / Organisation\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to store the extracted information\n",
        "    graph = {label: [] for label in property_labels.values()}\n",
        "    attributes = {attribute: [] for attribute in attribute_labels.values()}\n",
        "\n",
        "    # Process the SPARQL result\n",
        "    for binding in results['results']['bindings']:\n",
        "      prop = binding['property']['value'].split('/')[-1]  # Extract property ID from URI\n",
        "      value_label = binding.get('valueLabel', {}).get('value', None)\n",
        "      if value_label.startswith(\"statement/\"):\n",
        "        value_label = None\n",
        "      if (prop in property_labels or prop in attribute_labels) and value_label:\n",
        "        readable_label = property_labels.get(prop) or attribute_labels.get(prop)\n",
        "        if prop in property_labels:\n",
        "          graph[readable_label].append(value_label)\n",
        "        elif prop in attribute_labels:\n",
        "          attributes[readable_label].append(value_label)\n",
        "\n",
        "    graph = clean_dict(graph)\n",
        "    graph = fix_links(graph)\n",
        "    graph['attributes'] = clean_dict(attributes)\n",
        "\n",
        "    return graph\n",
        "  except Exception as e:\n",
        "    print(f\"Problem encountered {e}\")\n",
        "    return None\n",
        "\n",
        "# graph = get_knowledge_graph(q_number)\n",
        "\n",
        "# for key, value in graph.items():\n",
        "#   print(f\"{key}: {value}\\n\")"
      ],
      "metadata": {
        "id": "EXjxOEA5sRQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def generate_knowledge_panel(query: str, language: str = \"en\") -> dict:\n",
        "  \"\"\"\n",
        "  Generates a Knowledge Panel/Graph for the given entity (query)\n",
        "\n",
        "  Parameters:\n",
        "  query (str): The search query to find the entity on Wikidata.\n",
        "\n",
        "  Returns:\n",
        "  dict: A dictionary of knowledge graph with image, label, description and attributes\n",
        "  \"\"\"\n",
        "  start_time = time.time()\n",
        "  entity = search_entity(query, language)\n",
        "\n",
        "  if not entity:\n",
        "    return None\n",
        "\n",
        "  q_number = entity['id']\n",
        "  label = entity['display']['label']['value']\n",
        "  description = entity['display']['description']['value']\n",
        "\n",
        "  graph = get_knowledge_graph(q_number, language)\n",
        "\n",
        "  if not graph:\n",
        "    return None\n",
        "\n",
        "  graph['label'] = label\n",
        "  graph['description'] = description\n",
        "\n",
        "  print(f\"Generated Knowledge Panel in {time.time() - start_time} seconds\")\n",
        "\n",
        "  return graph"
      ],
      "metadata": {
        "id": "14aV7pK7sTnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_knowledge_panel('Seattle')"
      ],
      "metadata": {
        "id": "VnxQnLU2YcCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f007d5-0151-44f1-b62b-0c0591f117a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Knowledge Panel in 0.9962115287780762 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': 'http://commons.wikimedia.org/wiki/Special:FilePath/Seattle%20aerial%202%2C%20May%202023.png',\n",
              " 'website': 'https://www.seattle.gov/',\n",
              " 'social_links': {'twitter': 'https://twitter.com/CityofSeattle'},\n",
              " 'attributes': {'City': 'King County',\n",
              "  'Capital Of': 'King County',\n",
              "  'Area': ['369.243614', '369.466202'],\n",
              "  'Population': '737015',\n",
              "  'Head of Government': 'Bruce Harrell',\n",
              "  'Inception': '1851-11-13T00:00:00Z',\n",
              "  'Country': 'United States of America',\n",
              "  'Owner of': ['Climate Pledge Arena',\n",
              "   'Seattle City Hall',\n",
              "   'Seattle Center Monorail',\n",
              "   'First Hill Streetcar (Seattle)',\n",
              "   'Seattle Channel',\n",
              "   'South Lake Union Streetcar']},\n",
              " 'label': 'Seattle',\n",
              " 'description': 'city in and county seat of King County, Washington, United States'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SERPER API"
      ],
      "metadata": {
        "id": "4cL4DuFif3i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "serper_api_key = userdata.get('SERPER_API_KEY')\n",
        "\n",
        "def _serper_results(queries: list[str], search_type: str, n: int = 10):\n",
        "  \"\"\"\n",
        "  Performs a batch of serper searches for each keyword in the given list.\n",
        "\n",
        "  :param keywords: The list of keywords you would like to search\n",
        "  :param n: The number of results required from the search\n",
        "  :param search_type: websearch, image, or place\n",
        "\n",
        "  :returns: A list of search results\n",
        "  \"\"\"\n",
        "\n",
        "  url = \"/search\"\n",
        "  results_store = \"organic\"\n",
        "\n",
        "  if search_type == \"websearch\":\n",
        "    url = \"/search\"\n",
        "    results_store = \"organic\"\n",
        "  elif search_type == \"image\":\n",
        "    url = \"/images\"\n",
        "    results_store = \"images\"\n",
        "  elif search_type == \"place\":\n",
        "    url = \"/places\"\n",
        "    results_store = \"places\"\n",
        "  elif search_type == \"news\":\n",
        "    url = \"/news\"\n",
        "    results_store = \"news\"\n",
        "\n",
        "  try:\n",
        "    start_time = time.time()\n",
        "    conn = http.client.HTTPSConnection(\"google.serper.dev\")\n",
        "    payload = json.dumps([{\"q\": query} for query in queries])\n",
        "    headers = {\n",
        "      'X-API-KEY': serper_api_key,\n",
        "      'Content-Type': 'application/json'\n",
        "    }\n",
        "    conn.request(\"POST\", url, payload, headers)\n",
        "    res = conn.getresponse()\n",
        "    data = res.read()\n",
        "    batch_result = json.loads(data.decode(\"utf-8\"))\n",
        "\n",
        "    results = []\n",
        "    for result in batch_result:\n",
        "      results += result[results_store]\n",
        "\n",
        "    results = sorted(results, key=lambda x: x['position'])[:n]\n",
        "\n",
        "    print(f\"Total time taken to perform batch search: {time.time() - start_time} seconds\")\n",
        "\n",
        "    return results\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to perform a serper search {e}\")\n",
        "    return None\n",
        "\n",
        "def web_search(queries: list[str], n: int = 10):\n",
        "  \"\"\"\n",
        "  Performs a batch of web searches for each keyword in the given list.\n",
        "\n",
        "  :param keywords: The list of keywords you would like to search\n",
        "  :param n: The number of results required from the search\n",
        "\n",
        "  :returns: A list of search results\n",
        "  \"\"\"\n",
        "  try:\n",
        "    results = _serper_results(queries, \"websearch\", n)\n",
        "\n",
        "    return results\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to perform a web search {e}\")\n",
        "    return None\n",
        "\n",
        "def image_search(queries: list[str], n: int = 10):\n",
        "  \"\"\"\n",
        "  Performs a batch of image searches for each keyword in the given list.\n",
        "\n",
        "  :param keywords: The list of keywords you would like to search\n",
        "  :param n: The number of results required from the search\n",
        "\n",
        "  :returns: A list of image search results\n",
        "  \"\"\"\n",
        "  try:\n",
        "    results = _serper_results(queries, \"image\", n)\n",
        "\n",
        "    processed_results = []\n",
        "\n",
        "    for result in results:\n",
        "      processed_results.append({\n",
        "        'image': {\n",
        "          'url': result['imageUrl'],\n",
        "          'height': result['imageHeight'],\n",
        "          'width': result['imageWidth']\n",
        "        },\n",
        "        'title': result['title'],\n",
        "        'link': result['link']\n",
        "      })\n",
        "\n",
        "    return processed_results\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to perform an image search {e}\")\n",
        "    return None\n",
        "\n",
        "def place_search(queries: list[str], n: int = 10):\n",
        "  \"\"\"\n",
        "  Performs a batch of places searches for each keyword in the given list.\n",
        "\n",
        "  :param keywords: The list of keywords you would like to search\n",
        "  :param n: The number of results required from the search\n",
        "\n",
        "  :returns: A list of places search results\n",
        "  \"\"\"\n",
        "  try:\n",
        "    results = _serper_results(queries, \"place\", n)\n",
        "\n",
        "    processed_results = []\n",
        "\n",
        "    for result in results:\n",
        "      processed_results.append({\n",
        "        'title': result.get('title'),\n",
        "        'website': result.get('website', None),\n",
        "        'category': result.get('category'),\n",
        "        'location': {\n",
        "          'address': result.get('address'),\n",
        "          'latitude': result.get('latitude'),\n",
        "          'longitude': result.get('longitude')\n",
        "        },\n",
        "        'rating': {\n",
        "          'stars': result.get('rating'),\n",
        "          'count': result.get('ratingCount')\n",
        "        } if 'rating' in result else None,\n",
        "        'image': {\n",
        "          'url': result.get('thumbnailUrl'),\n",
        "        } if 'thumbnailUrl' in result else None,\n",
        "      })\n",
        "\n",
        "    return processed_results\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to perform a place search {e}\")\n",
        "    return None\n",
        "\n",
        "def news_search(queries: list[str], n: int = 10):\n",
        "  \"\"\"\n",
        "  Performs a batch of places news searches for each keyword in the given list.\n",
        "\n",
        "  :param keywords: The list of keywords you would like to search\n",
        "  :param n: The number of results required from the search\n",
        "\n",
        "  :returns: A list of places search results\n",
        "  \"\"\"\n",
        "  try:\n",
        "    results = _serper_results(queries, \"news\", n)\n",
        "\n",
        "    processed_results = []\n",
        "\n",
        "    for result in results:\n",
        "      processed_results.append({\n",
        "        'title': result.get('title'),\n",
        "        'link': result.get('link', None),\n",
        "        'date': result.get('date'),\n",
        "        'image': result.get('imageUrl'),\n",
        "        'source': result.get('source')\n",
        "      })\n",
        "\n",
        "    return processed_results\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to perform a news search {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "HqlE1_0GHAub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crawler"
      ],
      "metadata": {
        "id": "vuYeYs7bf5vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import time\n",
        "from newspaper import Article\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "class TimeoutException(Exception):\n",
        "  pass\n",
        "\n",
        "def thread_target(url, result_container):\n",
        "  try:\n",
        "    # Fetch the web page\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
        "      tag.decompose()\n",
        "\n",
        "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "      comment.extract()\n",
        "\n",
        "    for tag in soup.find_all(['div', 'section'], {'class': ['advertisement', 'promo', 'ad-banner']}):\n",
        "      tag.decompose()\n",
        "\n",
        "    clean_text = soup.get_text(separator=' ', strip=True)\n",
        "\n",
        "    result_container.append(clean_text)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    result_container.append(None)\n",
        "\n",
        "def extract_relevant_text(url):\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Container for the result (clean text)\n",
        "  result_container = []\n",
        "\n",
        "  # Create and start the thread\n",
        "  thread = Thread(target=thread_target, args=(url, result_container))\n",
        "  thread.start()\n",
        "\n",
        "  # Wait for the thread to complete or timeout\n",
        "  thread.join(timeout=2)\n",
        "\n",
        "  # Check if the thread has finished execution\n",
        "  if thread.is_alive():\n",
        "    print(\"Operation timed out\")\n",
        "    return None\n",
        "  else:\n",
        "    # Calculate time taken\n",
        "    end_time = time.time()\n",
        "    print(f\"Time taken to execute: {end_time - start_time} seconds\")\n",
        "    return result_container[0] if result_container else None\n",
        "\n",
        "def get_website_content(url):\n",
        "  try:\n",
        "    start_time = time.time()  # Start timing\n",
        "\n",
        "    # Create an Article object\n",
        "    article = Article(url)\n",
        "\n",
        "    # Download the article content\n",
        "    article.download()\n",
        "\n",
        "    # Parse the article\n",
        "    article.parse()\n",
        "\n",
        "    # Optionally, you can also perform natural language processing on the article\n",
        "    # article.nlp()\n",
        "\n",
        "    end_time = time.time()  # End timing\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Crawled page in: {execution_time} seconds\")\n",
        "\n",
        "    # Return the article's main text\n",
        "    return article.text\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred while fetching content from {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "def crawl(urls):\n",
        "  result = {}\n",
        "\n",
        "  start_time = time.time()  # Record the start time of the entire operation\n",
        "\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    future_to_url = {executor.submit(extract_relevant_text, url): url for url in urls}\n",
        "    for future in concurrent.futures.as_completed(future_to_url):\n",
        "      url = future_to_url[future]\n",
        "      try:\n",
        "        content = future.result()  # Get the content from the future\n",
        "        if content:\n",
        "          result[url] = content\n",
        "      except Exception as e:\n",
        "        print(f\"An error occurred while fetching content from {url}: {e}\")\n",
        "\n",
        "  end_time = time.time()  # Record the end time of the entire operation\n",
        "  total_time = end_time - start_time  # Calculate total execution time\n",
        "\n",
        "  print(f\"Total time taken to crawl {len(urls)} URLs: {total_time} seconds\")\n",
        "  return result"
      ],
      "metadata": {
        "id": "hfyq3DyFJsMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarizer"
      ],
      "metadata": {
        "id": "e6e84P5Qf7xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_web_search_summary_prompt = \"\"\"\n",
        "Objective: Summarize the provided search results into a coherent, concise response that directly answers the user's query. Your response should synthesize information from the given citations without repeating any words unnecessarily. Each piece of information you include from the citations must be followed by an appropriate citation marker to indicate its source.\n",
        "\n",
        "Context: You are provided with contents from multiple sources as follows:\n",
        "\n",
        "{context}\n",
        "\n",
        "----\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Read and Comprehend: Carefully read the content provided from each citation to understand the key points relevant to the user's query.\n",
        "\n",
        "Synthesize Information: Extract essential information, facts, or insights from the provided citations that directly address the user's query. Avoid including general or irrelevant details.\n",
        "\n",
        "Avoid Repetition: Do not repeat any words unnecessarily in your summary. Aim for varied vocabulary and succinct phrasing to convey the information clearly and efficiently.\n",
        "\n",
        "Incorporate Citations: After each piece of information derived from the sources, include an inline citation in the format [citation:X], where X corresponds to the source number. This citation should directly follow the sentence it supports. All the citation for a sentence shall appear at the end of the same sentence. For sentences with multiple citations, you are allowed to put them next to each other: [citation:1][citation:2]\n",
        "You can skip citations if there are no sources provided for the given context\n",
        "\n",
        "Answer from Context Only: Your summary must be strictly based on the information provided in the given context. Do not infer or add information not present in the citations.\n",
        "\n",
        "Clarity and Coherence: Ensure your summary is logical, coherent, and easy to understand. Structure your response to flow naturally, linking pieces of information in a way that forms a cohesive answer to the query.\n",
        "\n",
        "Markdown Syntax Usage Instructions: In addition to summarizing the search results, you are encouraged to format your answer using Markdown syntax to enhance readability and structure. You are allowed to empasise, underline, italicise text, use single level headlines, create blockquotes, create code blocks and horizontal rules only.\n",
        "For questions that can be answered in a few words or a single sentence, put that in a blockquote followed by a paragraph of explanation.\n",
        "\n",
        "**DO NOT put a list of citations/sources at the end. All citations must be made after each sentence, if applicable**\n",
        "\n",
        "Output Format:\n",
        "Your response should be formatted as a continuous paragraph or a list of bullet points, whichever best suits the synthesis of information from the provided sources. Remember to include citation markers appropriately.\n",
        "\n",
        "Example Query: \"What are the health benefits of green tea?\"\n",
        "\n",
        "Your Task: Using the content from [[citation:1]], [[citation:2]], and others, summarize the health benefits of green tea, including specific benefits mentioned in the sources. Remember to use citation markers after each benefit that you list based on the sources.\n",
        "\"\"\"\n",
        "\n",
        "stop_words = [\n",
        "  \"<|im_end|>\",\n",
        "  \"[End]\",\n",
        "  \"[end]\",\n",
        "  \"\\nReferences:\\n\",\n",
        "  \"\\nSources:\\n\",\n",
        "  \"End.\",\n",
        "]\n",
        "\n",
        "def summarise(content: str, query: str):\n",
        "  messages = [\n",
        "    # ChatMessage(role=\"system\", content=_web_search_summary_prompt.format(context=content)),\n",
        "    # ChatMessage(role=\"user\", content=query),\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": _web_search_summary_prompt.format(context=content)\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": query\n",
        "    },\n",
        "  ]\n",
        "\n",
        "  # response = mistral_client.chat_stream(\n",
        "  response = oai_client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo-0125',\n",
        "    messages=messages,\n",
        "    temperature=0.2,\n",
        "    stream=True,\n",
        "  )\n",
        "\n",
        "  summary = \"\"\n",
        "\n",
        "  for chunk in response:\n",
        "    text = chunk.choices[0].delta.content\n",
        "    if not text:\n",
        "      continue\n",
        "    summary += chunk.choices[0].delta.content\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "id": "wXIiT9FzPzgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "3T1dnUyff9S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_crawled_content(result: dict) -> str:\n",
        "  to_be_summarised = \"\"\n",
        "\n",
        "  tokens_used = 0\n",
        "\n",
        "  if result.get(\"sources\") or result.get(\"news\"):\n",
        "    links = [source.get('link') for source in result.get('sources') or result.get(\"news\")]\n",
        "    crawl_result = list(crawl(links).values())\n",
        "    for i, content in enumerate(crawl_result):\n",
        "      c_context = f\"---\\n[[citation:{i+1}]] {content}\\n\\n\"\n",
        "      c_tokens = count_tokens(c_context)\n",
        "      if c_tokens + tokens_used < MAX_ANSWER_TOKENS:\n",
        "        to_be_summarised += c_context\n",
        "        tokens_used += c_tokens\n",
        "\n",
        "  if result.get(\"places\"):\n",
        "    for place in result['places']:\n",
        "      for key, value in place.items():\n",
        "        to_be_summarised += f\"{key}: {value}\"\n",
        "\n",
        "  return to_be_summarised"
      ],
      "metadata": {
        "id": "os86UEmqP_fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "\n",
        "def search(query: str):\n",
        "  start_time = time.time()\n",
        "\n",
        "  result = {}\n",
        "\n",
        "  def execute_search_type():\n",
        "    return 'sources', web_search([query])\n",
        "\n",
        "  def execute_image_search():\n",
        "    return 'images', image_search([query])\n",
        "\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Submit each process as a separate task\n",
        "    futures = [\n",
        "      executor.submit(execute_search_type),\n",
        "      executor.submit(execute_image_search),\n",
        "    ]\n",
        "\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "      result_type, data = future.result()\n",
        "      if result_type:\n",
        "        result[result_type] = data\n",
        "\n",
        "  to_be_summarised = generate_crawled_content(result)\n",
        "\n",
        "  print(f\"Time to first token {time.time() - start_time} seconds\")\n",
        "\n",
        "  # if search_type != \"places\":\n",
        "  summary = summarise(to_be_summarised, query)\n",
        "  result['summary'] = summary\n",
        "\n",
        "  return result\n",
        "\n",
        "def search_pro(query: str, location: str | None = None):\n",
        "  start_time = time.time()\n",
        "  search_info = build_search(query, location)\n",
        "\n",
        "  keywords = search_info['keywords']\n",
        "  search_type = search_info['search_type']\n",
        "  search_images = search_info.get(\"image_search\", False)\n",
        "  entity = search_info.get('entity')\n",
        "\n",
        "  result = {}\n",
        "\n",
        "  def execute_search_type():\n",
        "    match search_type:\n",
        "      case 'websearch':\n",
        "        return 'sources', web_search(keywords)\n",
        "      case 'places':\n",
        "        return 'places', place_search(keywords)\n",
        "      case 'news':\n",
        "        return 'news', news_search(keywords)\n",
        "\n",
        "  def execute_image_search():\n",
        "    if search_images:\n",
        "      return 'images', image_search(keywords)\n",
        "    return None, None\n",
        "\n",
        "  def execute_knowledge_panel():\n",
        "    if entity:\n",
        "      return 'knowledge-panel', generate_knowledge_panel(entity)\n",
        "    return None, None\n",
        "\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # Submit each process as a separate task\n",
        "    futures = [\n",
        "      executor.submit(execute_search_type),\n",
        "      executor.submit(execute_image_search),\n",
        "      executor.submit(execute_knowledge_panel)\n",
        "    ]\n",
        "\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "      result_type, data = future.result()\n",
        "      if result_type:\n",
        "        result[result_type] = data\n",
        "\n",
        "  to_be_summarised = generate_crawled_content(result)\n",
        "\n",
        "  print(f\"Time to first token {time.time() - start_time} seconds\")\n",
        "\n",
        "  # if search_type != \"places\":\n",
        "  summary = summarise(to_be_summarised, query)\n",
        "  result['summary'] = summary\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "x8BciksgMKfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What’s the story of Aaron Swartz\"\n",
        "\n",
        "result = search(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avAcraATEBZB",
        "outputId": "0be11deb-f68c-448a-8e53-4eb95a523b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken to perform batch search: 0.7133047580718994 seconds\n",
            "Total time taken to perform batch search: 0.7772061824798584 seconds\n",
            "Time taken to execute: 0.5608060359954834 seconds\n",
            "Time taken to execute: 0.5964798927307129 seconds\n",
            "Time taken to execute: 0.3380565643310547 seconds\n",
            "Time taken to execute: 0.6799483299255371 seconds\n",
            "Time taken to execute: 1.3836181163787842 seconds\n",
            "Time taken to execute: 1.412717342376709 seconds\n",
            "Time taken to execute: 1.4436779022216797 seconds\n",
            "Time taken to execute: 0.9375176429748535 seconds\n",
            "Time taken to execute: 1.9414324760437012 seconds\n",
            "Total time taken to crawl 9 URLs: 1.94382643699646 seconds\n",
            "Time to first token 2.768836736679077 seconds\n",
            "Aaron Swartz was an American computer programmer, writer, political organizer, and Internet activist. He was known for his involvement in developing RSS and Reddit. Swartz was a passionate advocate for open access to information and fought against internet censorship, notably leading a campaign against the Stop Online Piracy Act (SOPA). Unfortunately, Swartz faced legal troubles after downloading academic articles from JSTOR through MIT's network, leading to a federal investigation and indictment. Despite JSTOR not pressing charges, the government pursued the case aggressively, which took a toll on Swartz's mental health. Ultimately, feeling overwhelmed by the legal battle and the fear of being labeled a felon, Swartz tragically took his own life in January 2013. His death sparked discussions about the criminal justice system and the prosecution of cybercrimes, leading to calls for legislative changes like \"Aaron's Law\" to prevent similar situations in the future. Swartz's story highlights the complexities of activism, mental health struggles, and the impact of legal battles on individuals. [[citation:4]][[citation:7]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.get('knowledge-panel')\n",
        "# result.get('places')"
      ],
      "metadata": {
        "id": "FdxQgBTsvvqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4P9M4SxBnxR",
        "outputId": "ad6a7388-8104-4b58-92c4-7a81a2abfc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['images', 'knowledge-panel', 'sources', 'summary'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}